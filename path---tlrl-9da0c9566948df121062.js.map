{"version":3,"sources":["webpack:///path---tlrl-9da0c9566948df121062.js","webpack:///./.cache/json/tlrl.json"],"names":["webpackJsonp","807","module","exports","data","post","id","html","htmlAst","type","children","tagName","properties","value","href","quirksMode","fields","slug","prefix","frontmatter","title","subTitle","cover","childImageSharp","resize","src","author","footnote","site","siteMetadata","facebook","appId","pathContext"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,MAAQC,GAAA,gGAAAC,KAAA,2rCAAixCC,SAAuCC,KAAA,OAAAC,WAA2BD,KAAA,UAAAE,QAAA,IAAAC,cAA8CF,WAAcD,KAAA,OAAAI,MAAA,0CAAgEJ,KAAA,UAAAE,QAAA,IAAAC,YAA6CE,KAAA,+CAAqDJ,WAAcD,KAAA,OAAAI,MAAA,YAAkCJ,KAAA,OAAAI,MAAA,OAA6BJ,KAAA,UAAAE,QAAA,IAAAC,YAA6CE,KAAA,gDAAsDJ,WAAcD,KAAA,OAAAI,MAAA,aAAmCJ,KAAA,OAAAI,MAAA,UAAgCJ,KAAA,UAAAE,QAAA,IAAAC,YAA6CE,KAAA,iCAAuCJ,WAAcD,KAAA,OAAAI,MAAA,eAAqCJ,KAAA,OAAAI,MAAA,OAA6BJ,KAAA,UAAAE,QAAA,IAAAC,cAA8CF,WAAcD,KAAA,OAAAI,MAAA,gBAAsCJ,KAAA,OAAAI,MAAA,OAA6BJ,KAAA,UAAAE,QAAA,IAAAC,cAA8CF,WAAcD,KAAA,OAAAI,MAAA,+7BAAm9BT,MAAUW,YAAA,IAAoBC,QAAWC,KAAA,SAAAC,OAAA,cAAsCC,aAAgBC,MAAA,6BAAAC,SAAA,yGAAAC,OAAkKC,iBAAmBC,QAAUC,IAAA,gEAAqEC,QAAWpB,GAAA,gFAAAC,KAAA,ijBAAgpBoB,UAAarB,GAAA,kFAAAC,KAAA,yeAAglBqB,MAASC,cAAgBC,UAAYC,MAAA,UAAiBC,aAAgBf,KAAA","file":"path---tlrl-9da0c9566948df121062.js","sourcesContent":["webpackJsonp([131664749312994],{\n\n/***/ 807:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"post\":{\"id\":\"/Users/jeremy/blog/content/posts/2017-12-01--tlrl/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<p>For more details: take a look at our <a href=\\\"/paper-e97ace3ccfc7fd208c37153ce774cb37.pdf\\\">paper</a>, <a href=\\\"/slides-fee8550a65711a69b373e261cec859c9.pdf\\\">slides</a> and <a href=\\\"https://github.com/TransferRL\\\">github</a></p>\\n<p>Abstract:</p>\\n<p>In model-free reinforcement learning it is typically challenging to solve complex or sparse-reward tasks tabula rasa. This paper modifies the method from Ammar et al. (Ammar et al., 2013) for transferring experience from one reinforcement learning task to another where the tasks differ in both their state and action spaces. The reliance on the unrealistic assumption of possessing a black-box model of the target task reward function is removed, and we introduce a method that transfers source task Q-values instead. We find that transferring source task Q-values achieves better transfer results than the black-box model based method for one experiment involving a goal state, sparse reward target task (2D Mountain Car to 3D Mountain Car), though this method results in negative transfer for two transfer experiments involving source and target tasks which have rewards differing in sign or scale (2D Mountain Car to CartPole; Acrobot to Cartpole).</p>\",\"htmlAst\":{\"type\":\"root\",\"children\":[{\"type\":\"element\",\"tagName\":\"p\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"For more details: take a look at our \"},{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"/paper-e97ace3ccfc7fd208c37153ce774cb37.pdf\"},\"children\":[{\"type\":\"text\",\"value\":\"paper\"}]},{\"type\":\"text\",\"value\":\", \"},{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"/slides-fee8550a65711a69b373e261cec859c9.pdf\"},\"children\":[{\"type\":\"text\",\"value\":\"slides\"}]},{\"type\":\"text\",\"value\":\" and \"},{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"https://github.com/TransferRL\"},\"children\":[{\"type\":\"text\",\"value\":\"github\"}]}]},{\"type\":\"text\",\"value\":\"\\n\"},{\"type\":\"element\",\"tagName\":\"p\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"Abstract:\"}]},{\"type\":\"text\",\"value\":\"\\n\"},{\"type\":\"element\",\"tagName\":\"p\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"In model-free reinforcement learning it is typically challenging to solve complex or sparse-reward tasks tabula rasa. This paper modifies the method from Ammar et al. (Ammar et al., 2013) for transferring experience from one reinforcement learning task to another where the tasks differ in both their state and action spaces. The reliance on the unrealistic assumption of possessing a black-box model of the target task reward function is removed, and we introduce a method that transfers source task Q-values instead. We find that transferring source task Q-values achieves better transfer results than the black-box model based method for one experiment involving a goal state, sparse reward target task (2D Mountain Car to 3D Mountain Car), though this method results in negative transfer for two transfer experiments involving source and target tasks which have rewards differing in sign or scale (2D Mountain Car to CartPole; Acrobot to Cartpole).\"}]}],\"data\":{\"quirksMode\":false}},\"fields\":{\"slug\":\"/tlrl/\",\"prefix\":\"2017-12-01\"},\"frontmatter\":{\"title\":\"RL using Transfer Learning\",\"subTitle\":\"Deep Transfer for Model-Free Reinforcement Learning Using Autonomous Intertask Mappings and Q-Learning\",\"cover\":{\"childImageSharp\":{\"resize\":{\"src\":\"/static/robot-f7445d8427e5111418c98c713271cf33-ada8c.jpg\"}}}}},\"author\":{\"id\":\"/Users/jeremy/blog/content/parts/author.md absPath of file >>> MarkdownRemark\",\"html\":\"<!-- **Mr. Gatsby** Proin ornare ligula eu tellus tempus elementum. Aenean [bibendum](/) iaculis mi, nec blandit lacus interdum vitae. Vestibulum non nibh risus, a scelerisque purus. :hearts: -->\\n<p><strong>Jeremy</strong> is currently a masterâ€™s student in University of Toronto. He is particularly interested in (but not limited by) Machine Learning, Computer Vision, Algorithms, Distributed Computing, App Development, and FPGA.\\nFollow him on <a href=\\\"https://github.com/junweima\\\">Github</a> and <a href=\\\"https://twitter.com/junweima2\\\">Twitter</a>.</p>\"},\"footnote\":{\"id\":\"/Users/jeremy/blog/content/parts/footnote.md absPath of file >>> MarkdownRemark\",\"html\":\"<ul>\\n<li>this is a demo site of the <a href=\\\"https://github.com/greglobinski/gatsby-starter-personal-blog\\\">gatsby-starter-personal-blog</a></li>\\n<li>built by <a href=\\\"https://www.greglobinski.com\\\">greg lobinski</a></li>\\n<li>GatsbyJS, ReactJs, CSS in JS - <a href=\\\"https://dev.greglobinski.com\\\">Front-end web development with Greg</a></li>\\n<li>deliverd by <a href=\\\"https://www.netlify.com/\\\">Netlify</a></li>\\n<li>photos by <a href=\\\"https://unsplash.com\\\">unsplash.com</a></li>\\n</ul>\"},\"site\":{\"siteMetadata\":{\"facebook\":{\"appId\":\"...\"}}}},\"pathContext\":{\"slug\":\"/tlrl/\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---tlrl-9da0c9566948df121062.js","module.exports = {\"data\":{\"post\":{\"id\":\"/Users/jeremy/blog/content/posts/2017-12-01--tlrl/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<p>For more details: take a look at our <a href=\\\"/paper-e97ace3ccfc7fd208c37153ce774cb37.pdf\\\">paper</a>, <a href=\\\"/slides-fee8550a65711a69b373e261cec859c9.pdf\\\">slides</a> and <a href=\\\"https://github.com/TransferRL\\\">github</a></p>\\n<p>Abstract:</p>\\n<p>In model-free reinforcement learning it is typically challenging to solve complex or sparse-reward tasks tabula rasa. This paper modifies the method from Ammar et al. (Ammar et al., 2013) for transferring experience from one reinforcement learning task to another where the tasks differ in both their state and action spaces. The reliance on the unrealistic assumption of possessing a black-box model of the target task reward function is removed, and we introduce a method that transfers source task Q-values instead. We find that transferring source task Q-values achieves better transfer results than the black-box model based method for one experiment involving a goal state, sparse reward target task (2D Mountain Car to 3D Mountain Car), though this method results in negative transfer for two transfer experiments involving source and target tasks which have rewards differing in sign or scale (2D Mountain Car to CartPole; Acrobot to Cartpole).</p>\",\"htmlAst\":{\"type\":\"root\",\"children\":[{\"type\":\"element\",\"tagName\":\"p\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"For more details: take a look at our \"},{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"/paper-e97ace3ccfc7fd208c37153ce774cb37.pdf\"},\"children\":[{\"type\":\"text\",\"value\":\"paper\"}]},{\"type\":\"text\",\"value\":\", \"},{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"/slides-fee8550a65711a69b373e261cec859c9.pdf\"},\"children\":[{\"type\":\"text\",\"value\":\"slides\"}]},{\"type\":\"text\",\"value\":\" and \"},{\"type\":\"element\",\"tagName\":\"a\",\"properties\":{\"href\":\"https://github.com/TransferRL\"},\"children\":[{\"type\":\"text\",\"value\":\"github\"}]}]},{\"type\":\"text\",\"value\":\"\\n\"},{\"type\":\"element\",\"tagName\":\"p\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"Abstract:\"}]},{\"type\":\"text\",\"value\":\"\\n\"},{\"type\":\"element\",\"tagName\":\"p\",\"properties\":{},\"children\":[{\"type\":\"text\",\"value\":\"In model-free reinforcement learning it is typically challenging to solve complex or sparse-reward tasks tabula rasa. This paper modifies the method from Ammar et al. (Ammar et al., 2013) for transferring experience from one reinforcement learning task to another where the tasks differ in both their state and action spaces. The reliance on the unrealistic assumption of possessing a black-box model of the target task reward function is removed, and we introduce a method that transfers source task Q-values instead. We find that transferring source task Q-values achieves better transfer results than the black-box model based method for one experiment involving a goal state, sparse reward target task (2D Mountain Car to 3D Mountain Car), though this method results in negative transfer for two transfer experiments involving source and target tasks which have rewards differing in sign or scale (2D Mountain Car to CartPole; Acrobot to Cartpole).\"}]}],\"data\":{\"quirksMode\":false}},\"fields\":{\"slug\":\"/tlrl/\",\"prefix\":\"2017-12-01\"},\"frontmatter\":{\"title\":\"RL using Transfer Learning\",\"subTitle\":\"Deep Transfer for Model-Free Reinforcement Learning Using Autonomous Intertask Mappings and Q-Learning\",\"cover\":{\"childImageSharp\":{\"resize\":{\"src\":\"/static/robot-f7445d8427e5111418c98c713271cf33-ada8c.jpg\"}}}}},\"author\":{\"id\":\"/Users/jeremy/blog/content/parts/author.md absPath of file >>> MarkdownRemark\",\"html\":\"<!-- **Mr. Gatsby** Proin ornare ligula eu tellus tempus elementum. Aenean [bibendum](/) iaculis mi, nec blandit lacus interdum vitae. Vestibulum non nibh risus, a scelerisque purus. :hearts: -->\\n<p><strong>Jeremy</strong> is currently a masterâ€™s student in University of Toronto. He is particularly interested in (but not limited by) Machine Learning, Computer Vision, Algorithms, Distributed Computing, App Development, and FPGA.\\nFollow him on <a href=\\\"https://github.com/junweima\\\">Github</a> and <a href=\\\"https://twitter.com/junweima2\\\">Twitter</a>.</p>\"},\"footnote\":{\"id\":\"/Users/jeremy/blog/content/parts/footnote.md absPath of file >>> MarkdownRemark\",\"html\":\"<ul>\\n<li>this is a demo site of the <a href=\\\"https://github.com/greglobinski/gatsby-starter-personal-blog\\\">gatsby-starter-personal-blog</a></li>\\n<li>built by <a href=\\\"https://www.greglobinski.com\\\">greg lobinski</a></li>\\n<li>GatsbyJS, ReactJs, CSS in JS - <a href=\\\"https://dev.greglobinski.com\\\">Front-end web development with Greg</a></li>\\n<li>deliverd by <a href=\\\"https://www.netlify.com/\\\">Netlify</a></li>\\n<li>photos by <a href=\\\"https://unsplash.com\\\">unsplash.com</a></li>\\n</ul>\"},\"site\":{\"siteMetadata\":{\"facebook\":{\"appId\":\"...\"}}}},\"pathContext\":{\"slug\":\"/tlrl/\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/tlrl.json\n// module id = 807\n// module chunks = 131664749312994"],"sourceRoot":""}